{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c35f684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we tweet clean to do away with things we dont need for our analysis\n",
    "#nltk is natural language tool kit\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2671536a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "346665db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenization is a way of splitting a string/text into tokens or words \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "767da922",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"These are 5 different words!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb08e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeFunc(tweet):\n",
    "    tokens = word_tokenize(tweet)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9c20d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n",
      "TWEET TOKENS:['These', 'are', '5', 'different', 'words', '!']\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenizeFunc(tweet)))\n",
    "print(type(tweet))\n",
    "\n",
    "print(\"TWEET TOKENS:{}\".format(tokenizeFunc(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbcf136a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stopwords in nltk is a commonly used word that a search engine has been programmed to ignore\n",
    "#words that are so common they are basically ignored by typical tokenizers\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords') \n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a575e5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'can', 'ma', 'hers', 'during', 'he', 'mightn', 'am', 'itself', 'below', \"couldn't\", 'that', 'wasn', 'than', 'what', 'her', 'himself', 'doing', 'being', 'have', 'why', \"you'd\", 'my', 'whom', 'were', 'be', 'own', \"should've\", 'so', \"shan't\", 'other', 's', 'isn', 'o', 'then', 'all', 'through', 'few', 'some', 'both', 'had', 'your', 'y', 'do', 've', 'we', 'down', 'after', 'more', \"wasn't\", 'she', 'they', \"hadn't\", \"aren't\", 'because', 'd', 'didn', 'their', 'before', 'ain', 'until', 'the', 'too', 'at', 'in', 'an', 'and', 'if', 'when', 'who', 'once', 'against', 'where', 'hasn', \"wouldn't\", 'by', 'very', 'but', \"don't\", 'most', 'themselves', 'no', 'him', 'shouldn', 'been', 'off', 'nor', 'as', 'not', 'ours', 'over', 'weren', 'just', 'with', 'to', \"she's\", 'any', 'them', 'are', 'those', 'for', 'doesn', 'haven', 'll', 'these', 'ourselves', 'a', 'mustn', \"you're\", \"hasn't\", 'did', 'yourself', 'on', 'about', \"it's\", 'of', 'yours', 'hadn', 'couldn', 'yourselves', \"you'll\", 'now', \"didn't\", 'shan', 'herself', 'm', \"weren't\", 'don', 'such', \"needn't\", \"mightn't\", 'you', 'i', \"haven't\", 'myself', \"you've\", 't', 'me', 'here', 'how', 'does', 'his', 'above', \"shouldn't\", 'again', 'aren', 'which', 'was', 'while', 'only', 're', 'will', 'having', 'its', 'this', 'each', 'there', 'into', 'needn', 'up', 'theirs', 'same', 'out', \"isn't\", 'further', 'from', 'it', \"mustn't\", 'is', \"won't\", \"that'll\", 'wouldn', 'or', 'has', 'between', 'should', 'under', \"doesn't\", 'our', 'won'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af334499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'can', 'ma', 'hers', 'during', 'he', 'mightn', 'am', 'itself', 'below', \"couldn't\", 'that', 'wasn', 'than', 'what', 'her', 'himself', 'doing', 'being', 'have', 'why', \"you'd\", 'my', 'whom', 'were', 'be', 'own', \"should've\", 'so', \"shan't\", 'other', 's', 'isn', 'o', 'then', 'all', 'through', 'few', 'some', 'both', 'had', 'your', 'y', 'do', 've', 'we', 'down', 'after', 'more', \"wasn't\", 'she', 'they', \"hadn't\", \"aren't\", 'because', 'd', 'didn', 'their', 'before', 'ain', 'until', 'the', 'too', 'at', 'in', 'an', 'and', 'if', 'when', 'who', 'once', 'against', 'where', 'hasn', \"wouldn't\", 'by', 'very', 'but', \"don't\", 'most', 'themselves', 'no', 'him', 'shouldn', 'been', 'off', 'nor', 'as', 'not', 'ours', 'over', 'weren', 'just', 'with', 'to', \"she's\", 'any', 'them', 'are', 'those', 'for', 'doesn', 'haven', 'll', 'these', 'ourselves', 'a', 'mustn', \"you're\", \"hasn't\", 'did', 'yourself', 'on', 'about', \"it's\", 'of', 'yours', 'hadn', 'couldn', 'yourselves', \"you'll\", \"didn't\", 'shan', 'herself', 'm', \"weren't\", 'don', 'such', \"needn't\", \"mightn't\", 'you', 'i', \"haven't\", 'myself', \"you've\", 't', 'me', 'here', 'how', 'does', 'his', 'above', \"shouldn't\", 'again', 'aren', 'which', 'was', 'while', 'only', 're', 'will', 'having', 'its', 'this', 'each', 'there', 'into', 'needn', 'up', 'theirs', 'same', 'out', \"isn't\", 'further', 'from', 'it', \"mustn't\", 'is', \"won't\", \"that'll\", 'wouldn', 'or', 'has', 'between', 'should', 'under', \"doesn't\", 'our', 'won'}\n"
     ]
    }
   ],
   "source": [
    "#to remove a word from the stopwords\n",
    "#we use the discard function and add when we want to add to the stopwords\n",
    "stop_words.discard('now')\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a72a1ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alnum means alphanumerical\n",
    "import string\n",
    "\n",
    "def custom_token(tweet, keep_alnum = False, keep_stop = False, keep_punct = False):\n",
    "    token_list = word_tokenize(tweet)\n",
    "    \n",
    "    if not keep_punct:\n",
    "        token_list = [token for token in token_list\n",
    "                     if token not in string.punctuation]\n",
    "    if not keep_alnum:\n",
    "        token_list = [token for token in token_list\n",
    "                     if token.isalpha()]\n",
    "    if not keep_stop:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stop_words.discard('now')\n",
    "        token_list = [token for token in token_list\n",
    "                     if not token in stop_words]\n",
    "            \n",
    "    return token_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc0ecc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet token:['These', 'are', '5', 'different', 'words', '!']\n",
      "Tweet token:['These', 'are', 'different', 'words']\n",
      "Tweet tokens:['These', '5', 'different', 'words']\n"
     ]
    }
   ],
   "source": [
    "print('Tweet token:{}'.format(custom_token(tweet, keep_alnum = True, keep_stop = True, keep_punct = True)))\n",
    "\n",
    "print('Tweet token:{}'.format(custom_token(tweet, keep_stop = True)))\n",
    "\n",
    "print('Tweet tokens:{}'.format(custom_token(tweet, keep_alnum = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c7be389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manager Management Managing\n",
    "#Manag Manag Manag\n",
    "\n",
    "#Over-stemming - reducing words\n",
    "#Universal university universe\n",
    "#Univers Univers Univers\n",
    "\n",
    "#Under-stemming\n",
    "# Alumnus Alumni Aluminae\n",
    "# Alumnu Alumni Alumina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86194d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmimg is finding the root word for a specific word as seen above: liking,liked - like(root word)\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a663bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\"Manager\", \"Management\", \"Managing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c4bf2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "snowball_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dac8bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens,stemmer):\n",
    "    token_list = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        token_list.append(stemmer.stem(token))\n",
    "    return token_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cb92145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter stem:['manag', 'manag', 'manag']\n",
      "Lancaster stem:['man', 'man', 'man']\n",
      "Snowball stem:['manag', 'manag', 'manag']\n"
     ]
    }
   ],
   "source": [
    "print('Porter stem:{}'.format(stem_tokens(tokens, porter_stemmer)))\n",
    "print('Lancaster stem:{}'.format(stem_tokens(tokens, lancaster_stemmer)))\n",
    "print('Snowball stem:{}'.format(stem_tokens(tokens, snowball_stemmer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ff25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokens2 = [\"international\",\"companies\",\"had\", \"interns\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98dc9a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter stem:['intern', 'compani', 'had', 'intern']\n",
      "Lancaster stem:['intern', 'company', 'had', 'intern']\n",
      "Snowball stem:['intern', 'compani', 'had', 'intern']\n"
     ]
    }
   ],
   "source": [
    "print('Porter stem:{}'.format(stem_tokens(tokens2, porter_stemmer)))\n",
    "print('Lancaster stem:{}'.format(stem_tokens(tokens2, lancaster_stemmer)))\n",
    "print('Snowball stem:{}'.format(stem_tokens(tokens2, snowball_stemmer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82f29b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lementization tries to maintian the logic of a sentence and words - we did not use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b656795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk. corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c3964f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2 = [\"international\",\"companies\",\"had\", \"interns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c6379ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_type ={\n",
    "    \n",
    "    \"international\":wordnet.ADJ,\n",
    "    \"companies\":wordnet.NOUN,\n",
    "    \"had\":wordnet.VERB,\n",
    "    \"interns\":wordnet.NOUN,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ca0023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c0d4ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens2,word_type,lemmatizer):\n",
    "    tokens_list = []\n",
    "    \n",
    "    for token in tokens2:\n",
    "        tokens_list.append(lemmatizer.lemmatize(token,word_type[token]))\n",
    "        \n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9d8ea00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet lemmatize:['international', 'company', 'have', 'intern']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tweet lemmatize:{}\".format(lemmatize_tokens(tokens2,word_type, lemmatizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f09c504a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "#regex is used to manipulate text data\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a41a80a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2dd0d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#windows and fullstop keys to get the emojis\n",
    "tweet = \"Ppm @TheResearcher: happy to research with you!!! 👍👍👍 https://www.w3schools.com #NLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75b81a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are replacing 'Ppm' with nothing because of the empty string\n",
    "def replace_retweet(tweet, default_replace=\"\"):\n",
    "    tweet = re.sub(\"Ppm\\s+\",default_replace,tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98078bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed tweet:@TheResearcher: happy to research with you!!! 👍👍👍 https://www.w3schools.com #NLP\n"
     ]
    }
   ],
   "source": [
    "print(\"processed tweet:{}\".format(replace_retweet(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7108d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove the user\n",
    "def replace_user(tweet,default_replace=\"User\"):\n",
    "\n",
    "    tweet = re.sub('\\B\\@\\w+',default_replace,tweet)\n",
    "    return tweet                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35a543b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed tweet:Ppm User: happy to research with you!!! 👍👍👍 https://www.w3schools.com #NLP\n"
     ]
    }
   ],
   "source": [
    "print(\"processed tweet:{}\".format(replace_user(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e31682c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\user\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c0946cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba2d6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#demojize is to remove the emoji from the tweet\n",
    "def demojize(tweet):\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f63c2cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed tweet:Ppm @TheResearcher: happy to research with you!!! :thumbs_up::thumbs_up::thumbs_up: https://www.w3schools.com #NLP\n"
     ]
    }
   ],
   "source": [
    "print(\"processed tweet:{}\".format(demojize(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b244a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to revove the url\n",
    "def replace_url(tweet,default_replace=\"\"):\n",
    "    tweet = re.sub('(http|https):\\/\\/\\S+',default_replace,tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e44e254f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed tweet:Ppm @TheResearcher: happy to research with you!!! 👍👍👍  #NLP\n"
     ]
    }
   ],
   "source": [
    "print(\"processed tweet:{}\".format(replace_url(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9411935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To remove a hashtag\n",
    "def replace_hashtag(tweet,default_replace=\"\"):\n",
    "    tweet = re.sub('#+', default_replace,tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d9ddfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed tweet:Ppm @TheResearcher: happy to research with you!!! 👍👍👍 https://www.w3schools.com NLP\n"
     ]
    }
   ],
   "source": [
    "print(\"processed tweet:{}\".format(replace_hashtag(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7590d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd0e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
